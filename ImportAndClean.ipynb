{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260a02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# clean text\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d554e",
   "metadata": {},
   "source": [
    "# Import and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd74d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend([\"'s\", \"b\", \"x\"])\n",
    "def clean_text(text):\n",
    "    # remove any @ mentions and extra white spaces\n",
    "    filtered_text = \" \".join(filter(lambda x: x[0]!='@', text.split())).strip()\n",
    "    \n",
    "    # remove URLs\n",
    "    filtered_text = \" \".join([x for x in filtered_text.split() if 'http' not in x])\n",
    "    \n",
    "    # remove special characters/numbers and convert to lowercase\n",
    "    filtered_text = re.sub(\"[^a-zA-Z]\", ' ', filtered_text).lower()\n",
    "    \n",
    "    # fix characters that repeat 3+ times\n",
    "    filtered_text = re.sub(r'(.)\\1+', r'\\1\\1', filtered_text)\n",
    "    \n",
    "    # expand contractions\n",
    "    filtered_text = \" \".join([contractions.fix(x) for x in filtered_text.split()])\n",
    "    \n",
    "    # transform words into root words\n",
    "    string = nlp(filtered_text)\n",
    "    filtered_text = \" \".join([word.lemma_.lower() for word in string])\n",
    "    \n",
    "    # remove stop words\n",
    "    filtered_text = \" \".join([word for word in filtered_text.split() if not word in stop_words])\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b82bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Opiates data\n",
    "df1 = pd.read_csv('SubredditDatasets/opiates.csv')\n",
    "opiates = df1[(df1['body'].notnull()) & (df1['created']<=1622519999)].drop_duplicates().reset_index(drop=True).copy()\n",
    "opiates['date'] = pd.to_datetime(opiates['created'].apply(lambda x: datetime.fromtimestamp(x).strftime('%Y-%m-%d')))\n",
    "opiates = opiates.drop(columns=['title', 'created', 'id', 'url'])\n",
    "\n",
    "# clean dataset\n",
    "opiates['cleaned_text'] = opiates['body'].apply(clean_text)\n",
    "\n",
    "# add tokens column\n",
    "opiates['tokens'] = [post.split() for post in opiates['cleaned_text']]\n",
    "opiates['post_length'] = opiates['tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c8122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Opiates Recovery data\n",
    "df2 = pd.read_csv('SubredditDatasets/recovery.csv')\n",
    "recovery = df2[(df2['body'].notnull()) & (df2['created']<=1622519999)].drop_duplicates().reset_index(drop=True).copy()\n",
    "recovery['date'] = pd.to_datetime(recovery['created'].apply(lambda x: datetime.fromtimestamp(x).strftime('%Y-%m-%d')))\n",
    "recovery = recovery.drop(columns=['title','created', 'id', 'url'])\n",
    "\n",
    "# clean dataset\n",
    "recovery['cleaned_text'] = recovery['body'].apply(clean_text)\n",
    "\n",
    "# add tokens column\n",
    "recovery['tokens'] = [post.split() for post in recovery['cleaned_text']]\n",
    "recovery['post_length'] = recovery['tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6382abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opiates = opiates[opiates['cleaned_text']!='']\n",
    "recovery = recovery[recovery['cleaned_text']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e969475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opiates.to_csv('SubredditDatasets/opiates_cleaned.csv', index=False)\n",
    "recovery.to_csv('SubredditDatasets/recovery_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
